import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import os
import re

current_dir = os.path.dirname(__file__)
save_dir = os.path.join(current_dir, '..', '..', 'resources', 'crawler')
save_dir = os.path.abspath(save_dir)
os.makedirs(save_dir, exist_ok=True)

base_url = "https://www.fmkorea.com"
headers = {
    "User-Agent": "Mozilla/5.0"
}
stop_flag = False
page = 1
deals = []

while not stop_flag:
    print(f"FMì½”ë¦¬ì•„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
    url = f"{base_url}/index.php?mid=hotdeal&page={page}"
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    for item in soup.select("li.li"):
        try:
            link_tag = item.select_one("a.hotdeal_var8")
            relative_link = link_tag['href'] if link_tag else ""
            full_url = base_url + relative_link

            # ìƒì„¸ í˜ì´ì§€ ìš”ì²­
            detail_res = requests.get(full_url, headers=headers)
            detail_soup = BeautifulSoup(detail_res.text, "html.parser")

            # ì œëª©
            title_tag = detail_soup.select_one("tr th:contains('ìƒí’ˆëª…') + td .xe_content")
            title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

            # ì™¸ë¶€ ë§í¬
            link_tag = detail_soup.select_one("a.hotdeal_url")
            external_url = link_tag['href'] if link_tag else full_url

            # ì‡¼í•‘ëª°
            site_tag = detail_soup.select_one("tr th:contains('ì‡¼í•‘ëª°') + td .xe_content")
            site = site_tag.get_text(strip=True).split()[0] if site_tag else "Unknown"

            # ê°€ê²©
            price_tag = detail_soup.select_one("tr th:contains('ê°€ê²©') + td .xe_content")
            price_text = price_tag.get_text(strip=True) if price_tag else None
            price_match = re.search(r'(\d+(,\d{3})*)ì›', price_text) if price_text else None
            price = int(price_match.group(1).replace(',', '')) if price_match else None

            # ê²Œì‹œì¼
            date_tag = detail_soup.select_one("span.date.m_no")
            posted_at_str = date_tag.get_text(strip=True) if date_tag else None
            try:
                posted_at = datetime.strptime(posted_at_str, "%Y.%m.%d %H:%M")
            except:
                posted_at = datetime.now()

            # ì¶”ì²œìˆ˜
            rec_tag = detail_soup.select_one("div.side.fr span:contains('ì¶”ì²œ ìˆ˜') b")
            rec_text = rec_tag.get_text(strip=True) if rec_tag else "0"
            try:
                rec_score = int(rec_text)
            except:
                rec_score = 0

            # ì¼ì£¼ì¼ ì´ìƒ ì§€ë‚œ ê¸€ì´ë©´ ì¤‘ë‹¨
            today = datetime.now()
            if (today - posted_at).days >= 7:
                stop_flag = True
                print("ğŸ“› ì˜¤ë˜ëœ ê²Œì‹œê¸€ ê°ì§€ë¨ â†’ í¬ë¡¤ë§ ì¢…ë£Œ")
                break

            created_at = today.strftime("%Y-%m-%d %H:%M:%S")

            deal = {
                "title": title,
                "url": external_url,
                "price": price,
                "site": site,
                "posted_at": posted_at.strftime("%Y-%m-%d %H:%M:%S"),
                "created_at": created_at,
                "likes": rec_score
            }

            print(deal)
            deals.append(deal)

        except Exception as e:
            print("âŒ Error parsing item:", e)
            continue

    page += 1

try:
    with open(os.path.join(save_dir, "fmkorea_crawling.json"), "w", encoding="utf-8") as f:
        json.dump(deals, f, ensure_ascii=False, indent=2)
except Exception as e:
    print("JSON ì €ì¥ ì‹¤íŒ¨:", e)
